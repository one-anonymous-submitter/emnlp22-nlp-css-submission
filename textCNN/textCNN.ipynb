{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"textCNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOfGOvorby/4IB7aOqAOjX+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"93cbe1db1fb649e1afff802d8afdce32":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d76b0483719a417fbb23a7c41bb18d49","IPY_MODEL_3fedc4ec9a044505b5753db92c19243d","IPY_MODEL_d6b23db41bff4bcab1967f0b2efeb443"],"layout":"IPY_MODEL_3e75adf5a5b34830b5b9616a5334f520"}},"d76b0483719a417fbb23a7c41bb18d49":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b7981c2c1a649cbbaae0211e14805d0","placeholder":"​","style":"IPY_MODEL_0457369d13e54d7fbd758311cff47a69","value":""}},"3fedc4ec9a044505b5753db92c19243d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_11fd5ca61f6847ada2bbe102e27f616b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a49389ec9a604ea290d988fb42be868c","value":1}},"d6b23db41bff4bcab1967f0b2efeb443":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5bc1609a5a14dcf8feac1effc58e2c0","placeholder":"​","style":"IPY_MODEL_e7fe40d16caf4ec8bb119ad2a785ae78","value":" 2000000/? [00:37&lt;00:00, 56958.14it/s]"}},"3e75adf5a5b34830b5b9616a5334f520":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b7981c2c1a649cbbaae0211e14805d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0457369d13e54d7fbd758311cff47a69":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11fd5ca61f6847ada2bbe102e27f616b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"a49389ec9a604ea290d988fb42be868c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5bc1609a5a14dcf8feac1effc58e2c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7fe40d16caf4ec8bb119ad2a785ae78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3jPkF-FGGElf","executionInfo":{"status":"ok","timestamp":1652792364811,"user_tz":-120,"elapsed":18829,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}},"outputId":"e110d1ad-03e2-42d9-f115-8c5fa4024dda"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import re\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd\n","import nltk\n","nltk.download(\"all\")\n","import matplotlib.pyplot as plt\n","import torch\n","\n","%matplotlib inline"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xu2Upa-uGyJG","executionInfo":{"status":"ok","timestamp":1652792405664,"user_tz":-120,"elapsed":37427,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}},"outputId":"690b63c7-1e50-4c89-b981-502168c6740f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]}]},{"cell_type":"code","source":["#https://chriskhanhtran.github.io/_posts/2020-02-01-cnn-sentence-classification/\n","data = pd.read_csv(\"/content/drive/MyDrive/samplecoded4.csv\",encoding=\"utf-8\")\n","\n","dataa = pd.read_csv(\"/content/drive/MyDrive/round1c.csv\",encoding=\"utf-8\")\n","dataaa = pd.read_csv(\"/content/drive/MyDrive/round2fixc.csv\",encoding=\"utf-8\")\n","dataaaa = pd.read_csv(\"/content/drive/MyDrive/round3c.csv\",encoding=\"utf-8\")\n","\n","dataa = dataa[[\"ref_id\",\"text_o\",\"effectiveness\",\"risiko\",\"distribution\",\"mandatory\",\"politics\",\"pharma\",\"CT\",\"Child\"]]\n","dataaa = dataaa[[\"ref_id\",\"text_o\",\"effectiveness\",\"risiko\",\"distribution\",\"mandatory\",\"politics\",\"pharma\",\"CT\",\"Child\"]]\n","dataaaa = dataaaa[[\"ref_id\",\"text_o\",\"effectiveness\",\"risiko\",\"distribution\",\"mandatory\",\"politics\",\"pharma\",\"CT\",\"Child\"]]\n","\n","data = pd.concat([data, dataa],axis=0,ignore_index=True, sort=False)\n","data = pd.concat([data, dataaa],axis=0,ignore_index=True, sort=False)\n","data = pd.concat([data, dataaaa],axis=0,ignore_index=True, sort=False)\n","\n","\n","# Remove links\n","http_link_pattern = r'http\\S+'\n","bitly_link_pattern = r'bit.ly/\\S+'\n","data['cleaned'] = data['text_o'].str.replace(http_link_pattern, '')\n","data['cleaned'] = data['text_o'].str.replace(bitly_link_pattern, '')\n","\n","import random\n","random.seed(999)\n","codelist =  [\"effectiveness\",\"risiko\",\"distribution\",\"mandatory\",\"politics\",\"pharma\",\"CT\",\"Child\"]\n","cate = \"Child\"\n","\n","print(len(data[data[cate].map(lambda x: x == 1)].reset_index()))\n","print(len(data[data[cate].map(lambda x: x == 0)].reset_index()))\n","\n","data1 = data[data[cate].map(lambda x: x == 1)].reset_index()\n","data0 = data[data[cate].map(lambda x: x == 0)].reset_index()\n","\n","samplelen = len(data1)\n","ranlist = random.sample(range(len(data0)), samplelen)\n","data0 = data0.iloc[ranlist,:]\n","texts = np.array(data0[\"cleaned\"].values.tolist()+data1[\"cleaned\"].values.tolist())\n","labels = np.array(data0[cate].values.tolist()+data1[cate].values.tolist())\n","print(len(labels))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"axO9ueKoGuh3","executionInfo":{"status":"ok","timestamp":1652795802780,"user_tz":-120,"elapsed":698,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}},"outputId":"552b57ce-68c7-4655-b46e-788952b851e1"},"execution_count":232,"outputs":[{"output_type":"stream","name":"stdout","text":["168\n","2886\n","336\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n"]}]},{"cell_type":"code","source":["%%time\n","#URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\"\n","URL = \" https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.de.300.vec.gz\"\n","FILE = \"fastText\"\n","\n","if os.path.isdir(FILE):\n","    print(\"fastText exists.\")\n","else:\n","    !wget -P $FILE $URL\n","    !gunzip $FILE/cc.de.300.vec.gz -d $FILE"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZuncNmjG0eK","executionInfo":{"status":"ok","timestamp":1652795817832,"user_tz":-120,"elapsed":514,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}},"outputId":"a50e88b3-9597-4c6d-82f1-250a5ba9b507"},"execution_count":233,"outputs":[{"output_type":"stream","name":"stdout","text":["fastText exists.\n","CPU times: user 302 µs, sys: 0 ns, total: 302 µs\n","Wall time: 241 µs\n"]}]},{"cell_type":"code","source":["if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n","    print('Device name:', torch.cuda.get_device_name(0))\n","\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXAm_qBBJld5","executionInfo":{"status":"ok","timestamp":1652795818352,"user_tz":-120,"elapsed":13,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}},"outputId":"f9fecb49-809d-4f24-b35b-9b044d6469eb"},"execution_count":234,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","Device name: Tesla T4\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","from collections import defaultdict\n","\n","def tokenize(texts):\n","    \"\"\"Tokenize texts, build vocabulary and find maximum sentence length.\n","    \n","    Args:\n","        texts (List[str]): List of text data\n","    \n","    Returns:\n","        tokenized_texts (List[List[str]]): List of list of tokens\n","        word2idx (Dict): Vocabulary built from the corpus\n","        max_len (int): Maximum sentence length\n","    \"\"\"\n","\n","    max_len = 0\n","    tokenized_texts = []\n","    word2idx = {}\n","\n","    # Add <pad> and <unk> tokens to the vocabulary\n","    word2idx['<pad>'] = 0\n","    word2idx['<unk>'] = 1\n","\n","    # Building our vocab from the corpus starting from index 2\n","    idx = 2\n","    for sent in texts:\n","        tokenized_sent = word_tokenize(sent)\n","\n","        # Add `tokenized_sent` to `tokenized_texts`\n","        tokenized_texts.append(tokenized_sent)\n","\n","        # Add new token to `word2idx`\n","        for token in tokenized_sent:\n","            if token not in word2idx:\n","                word2idx[token] = idx\n","                idx += 1\n","\n","        # Update `max_len`\n","        max_len = max(max_len, len(tokenized_sent))\n","\n","    return tokenized_texts, word2idx, max_len\n","\n","def encode(tokenized_texts, word2idx, max_len):\n","    \"\"\"Pad each sentence to the maximum sentence length and encode tokens to\n","    their index in the vocabulary.\n","\n","    Returns:\n","        input_ids (np.array): Array of token indexes in the vocabulary with\n","            shape (N, max_len). It will the input of our CNN model.\n","    \"\"\"\n","\n","    input_ids = []\n","    for tokenized_sent in tokenized_texts:\n","        # Pad sentences to max_len\n","        tokenized_sent += ['<pad>'] * (max_len - len(tokenized_sent))\n","\n","        # Encode tokens to input_ids\n","        input_id = [word2idx.get(token) for token in tokenized_sent]\n","        input_ids.append(input_id)\n","    \n","    return np.array(input_ids)"],"metadata":{"id":"34V4Kl6RK4gW","executionInfo":{"status":"ok","timestamp":1652795819095,"user_tz":-120,"elapsed":4,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}}},"execution_count":235,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm_notebook\n","\n","def load_pretrained_vectors(word2idx, fname):\n","    \"\"\"Load pretrained vectors and create embedding layers.\n","    \n","    Args:\n","        word2idx (Dict): Vocabulary built from the corpus\n","        fname (str): Path to pretrained vector file\n","\n","    Returns:\n","        embeddings (np.array): Embedding matrix with shape (N, d) where N is\n","            the size of word2idx and d is embedding dimension\n","    \"\"\"\n","\n","    print(\"Loading pretrained vectors...\")\n","    fin = open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n","    n, d = map(int, fin.readline().split())\n","\n","    # Initilize random embeddings\n","    embeddings = np.random.uniform(-0.25, 0.25, (len(word2idx), d))\n","    embeddings[word2idx['<pad>']] = np.zeros((d,))\n","\n","    # Load pretrained vectors\n","    count = 0\n","    for line in tqdm_notebook(fin):\n","        tokens = line.rstrip().split(' ')\n","        word = tokens[0]\n","        if word in word2idx:\n","            count += 1\n","            embeddings[word2idx[word]] = np.array(tokens[1:], dtype=np.float32)\n","\n","    print(f\"There are {count} / {len(word2idx)} pretrained vectors found.\")\n","\n","    return embeddings\n"],"metadata":{"id":"v1hzyVOeLDht","executionInfo":{"status":"ok","timestamp":1652795819593,"user_tz":-120,"elapsed":4,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}}},"execution_count":236,"outputs":[]},{"cell_type":"code","source":["print(\"Tokenizing...\\n\")\n","tokenized_texts, word2idx, max_len = tokenize(texts)\n","input_ids = encode(tokenized_texts, word2idx, max_len)\n","\n","# Load pretrained vectors\n","embeddings = load_pretrained_vectors(word2idx, \"fastText/cc.de.300.vec\")\n","embeddings = torch.tensor(embeddings)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":151,"referenced_widgets":["93cbe1db1fb649e1afff802d8afdce32","d76b0483719a417fbb23a7c41bb18d49","3fedc4ec9a044505b5753db92c19243d","d6b23db41bff4bcab1967f0b2efeb443","3e75adf5a5b34830b5b9616a5334f520","0b7981c2c1a649cbbaae0211e14805d0","0457369d13e54d7fbd758311cff47a69","11fd5ca61f6847ada2bbe102e27f616b","a49389ec9a604ea290d988fb42be868c","d5bc1609a5a14dcf8feac1effc58e2c0","e7fe40d16caf4ec8bb119ad2a785ae78"]},"id":"hUeMrMS5LLls","executionInfo":{"status":"ok","timestamp":1652795858533,"user_tz":-120,"elapsed":38446,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}},"outputId":"318438e1-696f-4241-b05f-63b3fbaba159"},"execution_count":237,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizing...\n","\n","Loading pretrained vectors...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93cbe1db1fb649e1afff802d8afdce32"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["There are 2915 / 3829 pretrained vectors found.\n"]}]},{"cell_type":"code","source":["from torch.utils.data import (TensorDataset, DataLoader, RandomSampler,\n","                              SequentialSampler)\n","\n","def data_loader(train_inputs, val_inputs, train_labels, val_labels,\n","                batch_size=50):\n","    \"\"\"Convert train and validation sets to torch.Tensors and load them to\n","    DataLoader.\n","    \"\"\"\n","\n","    # Convert data type to torch.Tensor\n","    train_inputs, val_inputs, train_labels, val_labels =\\\n","    tuple(torch.tensor(data) for data in\n","          [train_inputs, val_inputs, train_labels, val_labels])\n","\n","    # Specify batch_size\n","    batch_size = 50\n","\n","    # Create DataLoader for training data\n","    train_data = TensorDataset(train_inputs, train_labels)\n","    train_sampler = RandomSampler(train_data)\n","    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","    # Create DataLoader for validation data\n","    val_data = TensorDataset(val_inputs, val_labels)\n","    val_sampler = SequentialSampler(val_data)\n","    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n","\n","    return train_dataloader, val_dataloader"],"metadata":{"id":"SaqLcGoULOt3","executionInfo":{"status":"ok","timestamp":1652795858534,"user_tz":-120,"elapsed":34,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}}},"execution_count":238,"outputs":[]},{"cell_type":"code","source":["#from sklearn.model_selection import train_test_split\n","\n","# Train Test Split\n","#train_inputs, val_inputs, train_labels, val_labels = train_test_split(\n","#    input_ids, labels, test_size=0.1, random_state=42)\n","\n","# Load data to PyTorch DataLoader\n","#train_dataloader, val_dataloader = \\\n","#data_loader(train_inputs, val_inputs, train_labels, val_labels, batch_size=50)\n","\n","#print(train_inputs.shape)\n"],"metadata":{"id":"zz_Djx8SLiSC","executionInfo":{"status":"ok","timestamp":1652795858537,"user_tz":-120,"elapsed":33,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}}},"execution_count":239,"outputs":[]},{"cell_type":"code","source":["#from sklearn.utils import shuffle\n","#input_ids2, label2 = shuffle(input_ids, labels, random_state=42)\n","#print(input_ids2.shape)\n","# cross validation\n","from sklearn.model_selection import KFold\n","kf = KFold(n_splits=10, random_state=42, shuffle=True)\n","\n","#til = []\n","#vil = []\n","#tll = []\n","#vll = []\n","tdl = []\n","vll = []\n","for train, test in kf.split(input_ids):\n","    train_inputs, val_inputs, train_labels, val_labels = input_ids[train], input_ids[test], labels[train], labels[test]\n","    #til.append(train_inputs)\n","    #vil.append(val_inputs)\n","    #tll.append(train_labels)\n","    #vll.append(val_labels)\n","\n","    train_dataloader, val_dataloader = \\\n","    data_loader(train_inputs, val_inputs, train_labels, val_labels, batch_size=50)\n","\n","    tdl.append(train_dataloader)\n","    vll.append(val_dataloader)\n","    #print(\"%s %s %s %s\" % (train_inputs.shape, val_inputs.shape, train_labels.shape, val_labels.shape))\n","print(vll[0],tdl[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0uoe5MSVzfb","executionInfo":{"status":"ok","timestamp":1652795858538,"user_tz":-120,"elapsed":33,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}},"outputId":"2f1fc5cb-b378-4135-b973-d0af4a2ad95e"},"execution_count":240,"outputs":[{"output_type":"stream","name":"stdout","text":["<torch.utils.data.dataloader.DataLoader object at 0x7fccc1aba450> <torch.utils.data.dataloader.DataLoader object at 0x7fccc1abaf90>\n"]}]},{"cell_type":"code","source":["#filter_sizes = [2, 3, 4]\n","#num_filters = [2, 2, 2]"],"metadata":{"id":"Va0HV48vLlEX","executionInfo":{"status":"ok","timestamp":1652795858539,"user_tz":-120,"elapsed":30,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}}},"execution_count":241,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class CNN_NLP(nn.Module):\n","    \"\"\"An 1D Convulational Neural Network for Sentence Classification.\"\"\"\n","    def __init__(self,\n","                 pretrained_embedding=None,\n","                 freeze_embedding=False,\n","                 vocab_size=None,\n","                 embed_dim=300,\n","                 filter_sizes=[3, 4, 5],\n","                 num_filters=[100, 100, 100],\n","                 num_classes=2,\n","                 dropout=0.5):\n","        \"\"\"\n","        The constructor for CNN_NLP class.\n","\n","        Args:\n","            pretrained_embedding (torch.Tensor): Pretrained embeddings with\n","                shape (vocab_size, embed_dim)\n","            freeze_embedding (bool): Set to False to fine-tune pretraiend\n","                vectors. Default: False\n","            vocab_size (int): Need to be specified when not pretrained word\n","                embeddings are not used.\n","            embed_dim (int): Dimension of word vectors. Need to be specified\n","                when pretrained word embeddings are not used. Default: 300\n","            filter_sizes (List[int]): List of filter sizes. Default: [3, 4, 5]\n","            num_filters (List[int]): List of number of filters, has the same\n","                length as `filter_sizes`. Default: [100, 100, 100]\n","            n_classes (int): Number of classes. Default: 2\n","            dropout (float): Dropout rate. Default: 0.5\n","        \"\"\"\n","\n","        super(CNN_NLP, self).__init__()\n","        # Embedding layer\n","        if pretrained_embedding is not None:\n","            self.vocab_size, self.embed_dim = pretrained_embedding.shape\n","            self.embedding = nn.Embedding.from_pretrained(pretrained_embedding,\n","                                                          freeze=freeze_embedding)\n","        else:\n","            self.embed_dim = embed_dim\n","            self.embedding = nn.Embedding(num_embeddings=vocab_size,\n","                                          embedding_dim=self.embed_dim,\n","                                          padding_idx=0,\n","                                          max_norm=5.0)\n","        # Conv Network\n","        self.conv1d_list = nn.ModuleList([\n","            nn.Conv1d(in_channels=self.embed_dim,\n","                      out_channels=num_filters[i],\n","                      kernel_size=filter_sizes[i])\n","            for i in range(len(filter_sizes))\n","        ])\n","        # Fully-connected layer and Dropout\n","        self.fc = nn.Linear(np.sum(num_filters), num_classes)\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","    def forward(self, input_ids):\n","        \"\"\"Perform a forward pass through the network.\n","\n","        Args:\n","            input_ids (torch.Tensor): A tensor of token ids with shape\n","                (batch_size, max_sent_length)\n","\n","        Returns:\n","            logits (torch.Tensor): Output logits with shape (batch_size,\n","                n_classes)\n","        \"\"\"\n","\n","        # Get embeddings from `input_ids`. Output shape: (b, max_len, embed_dim)\n","        x_embed = self.embedding(input_ids).float()\n","\n","        # Permute `x_embed` to match input shape requirement of `nn.Conv1d`.\n","        # Output shape: (b, embed_dim, max_len)\n","        x_reshaped = x_embed.permute(0, 2, 1)\n","\n","        # Apply CNN and ReLU. Output shape: (b, num_filters[i], L_out)\n","        x_conv_list = [F.relu(conv1d(x_reshaped)) for conv1d in self.conv1d_list]\n","\n","        # Max pooling. Output shape: (b, num_filters[i], 1)\n","        x_pool_list = [F.max_pool1d(x_conv, kernel_size=x_conv.shape[2])\n","            for x_conv in x_conv_list]\n","        \n","        # Concatenate x_pool_list to feed the fully connected layer.\n","        # Output shape: (b, sum(num_filters))\n","        x_fc = torch.cat([x_pool.squeeze(dim=2) for x_pool in x_pool_list],\n","                         dim=1)\n","        \n","        # Compute logits. Output shape: (b, n_classes)\n","        logits = self.fc(self.dropout(x_fc))\n","\n","        return logits"],"metadata":{"id":"CoI2oUIkLnTS","executionInfo":{"status":"ok","timestamp":1652795858539,"user_tz":-120,"elapsed":29,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}}},"execution_count":242,"outputs":[]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","def initilize_model(pretrained_embedding=None,\n","                    freeze_embedding=False,\n","                    vocab_size=None,\n","                    embed_dim=300,\n","                    filter_sizes=[3, 4, 5],\n","                    num_filters=[100, 100, 100],\n","                    num_classes=2,\n","                    dropout=0.5,\n","                    learning_rate=0.01):\n","    \"\"\"Instantiate a CNN model and an optimizer.\"\"\"\n","\n","    assert (len(filter_sizes) == len(num_filters)), \"filter_sizes and \\\n","    num_filters need to be of the same length.\"\n","\n","    # Instantiate CNN model\n","    cnn_model = CNN_NLP(pretrained_embedding=pretrained_embedding,\n","                        freeze_embedding=freeze_embedding,\n","                        vocab_size=vocab_size,\n","                        embed_dim=embed_dim,\n","                        filter_sizes=filter_sizes,\n","                        num_filters=num_filters,\n","                        num_classes=2,\n","                        dropout=0.5)\n","    \n","    # Send model to `device` (GPU/CPU)\n","    cnn_model.to(device)\n","\n","    # Instantiate Adadelta optimizer\n","    optimizer = optim.Adadelta(cnn_model.parameters(),\n","                               lr=learning_rate,\n","                               rho=0.95)\n","\n","    return cnn_model, optimizer"],"metadata":{"id":"ctKQ7lFLLshu","executionInfo":{"status":"ok","timestamp":1652795858540,"user_tz":-120,"elapsed":29,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}}},"execution_count":243,"outputs":[]},{"cell_type":"code","source":["import random\n","import time\n","\n","# Specify loss function\n","loss_fn = nn.CrossEntropyLoss()\n","\n","def set_seed(seed_value=42):\n","    \"\"\"Set seed for reproducibility.\"\"\"\n","\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","\n","def train(model, optimizer, train_dataloader, val_dataloader=None, epochs=10):\n","    \"\"\"Train the CNN model.\"\"\"\n","    \n","    # Tracking best validation accuracy\n","    best_accuracy = 0\n","\n","    # Start training loop\n","    print(\"Start training...\\n\")\n","    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {\\\n","    'Val Acc':^9} | {'Elapsed':^9}\")\n","    print(\"-\"*60)\n","\n","    for epoch_i in range(epochs):\n","        # =======================================\n","        #               Training\n","        # =======================================\n","\n","        # Tracking time and loss\n","        t0_epoch = time.time()\n","        total_loss = 0\n","\n","        # Put the model into the training mode\n","        model.train()\n","\n","        for step, batch in enumerate(train_dataloader):\n","            # Load batch to GPU\n","            b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n","\n","            # Zero out any previously calculated gradients\n","            model.zero_grad()\n","\n","            # Perform a forward pass. This will return logits.\n","            logits = model(b_input_ids)\n","\n","            # Compute loss and accumulate the loss values\n","            loss = loss_fn(logits, b_labels)\n","            total_loss += loss.item()\n","\n","            # Perform a backward pass to calculate gradients\n","            loss.backward()\n","\n","            # Update parameters\n","            optimizer.step()\n","\n","        # Calculate the average loss over the entire training data\n","        avg_train_loss = total_loss / len(train_dataloader)\n","\n","        # =======================================\n","        #               Evaluation\n","        # =======================================\n","        if val_dataloader is not None:\n","            # After the completion of each training epoch, measure the model's\n","            # performance on our validation set.\n","            val_loss, val_accuracy = evaluate(model, val_dataloader)\n","\n","            # Track the best accuracy\n","            if val_accuracy > best_accuracy:\n","                best_accuracy = val_accuracy\n","\n","            # Print performance over the entire training data\n","            time_elapsed = time.time() - t0_epoch\n","            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {\\\n","            val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n","            \n","    print(\"\\n\")\n","    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n","\n","    #modified by me\n","    return best_accuracy\n","\n","def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's\n","    performance on our validation set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled\n","    # during the test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    val_accuracy = []\n","    val_loss = []\n","\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        b_input_ids, b_labels = tuple(t.to(device) for t in batch)\n","\n","        # Compute logits\n","        with torch.no_grad():\n","            logits = model(b_input_ids)\n","\n","        # Compute loss\n","        loss = loss_fn(logits, b_labels)\n","        val_loss.append(loss.item())\n","\n","        # Get the predictions\n","        preds = torch.argmax(logits, dim=1).flatten()\n","\n","        # Calculate the accuracy rate\n","        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n","        val_accuracy.append(accuracy)\n","\n","    # Compute the average accuracy and loss over the validation set.\n","    val_loss = np.mean(val_loss)\n","    val_accuracy = np.mean(val_accuracy)\n","\n","    return val_loss, val_accuracy"],"metadata":{"id":"Vsx1MyRqLwbh","executionInfo":{"status":"ok","timestamp":1652795858541,"user_tz":-120,"elapsed":29,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}}},"execution_count":244,"outputs":[]},{"cell_type":"code","source":["bestl= []\n","for i in range(10):\n","    set_seed(42)\n","    print(i+1,\"-fold--------------\")\n","    #print(vll[1],tdl[1])\n","\n","    cnn_non_static, optimizer = initilize_model(vocab_size=len(word2idx),\n","                                      embed_dim=300,\n","                                      learning_rate=0.25,\n","                                      dropout=0.5)\n","\n","    best_acc = train(cnn_non_static, optimizer, tdl[i], vll[i], epochs=20)\n","    bestl.append(best_acc)\n","\n","print(np.mean(np.array(bestl)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0yhBFGqmKHKU","executionInfo":{"status":"ok","timestamp":1652795866370,"user_tz":-120,"elapsed":7856,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}},"outputId":"a6bd61e6-2a32-4afb-a975-24459ba0a5d8"},"execution_count":245,"outputs":[{"output_type":"stream","name":"stdout","text":["1 -fold--------------\n","Start training...\n","\n"," Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","------------------------------------------------------------\n","   1    |   0.725874   |  0.727493  |   41.18   |   0.08   \n","   2    |   0.629617   |  0.764180  |   41.18   |   0.08   \n","   3    |   0.610252   |  0.696751  |   47.06   |   0.08   \n","   4    |   0.537592   |  0.650028  |   70.59   |   0.05   \n","   5    |   0.523483   |  0.643661  |   67.65   |   0.05   \n","   6    |   0.505806   |  0.639279  |   70.59   |   0.05   \n","   7    |   0.444228   |  0.639155  |   55.88   |   0.05   \n","   8    |   0.400481   |  0.604842  |   76.47   |   0.05   \n","   9    |   0.379834   |  0.588279  |   76.47   |   0.05   \n","  10    |   0.325274   |  0.646263  |   50.00   |   0.04   \n","  11    |   0.305973   |  0.594046  |   64.71   |   0.04   \n","  12    |   0.323966   |  0.556073  |   79.41   |   0.04   \n","  13    |   0.254629   |  0.563100  |   67.65   |   0.05   \n","  14    |   0.230918   |  0.567094  |   70.59   |   0.04   \n","  15    |   0.230643   |  0.585474  |   64.71   |   0.04   \n","  16    |   0.211202   |  0.573211  |   64.71   |   0.04   \n","  17    |   0.192476   |  0.583428  |   61.76   |   0.05   \n","  18    |   0.171389   |  0.584868  |   61.76   |   0.05   \n","  19    |   0.167542   |  0.523261  |   70.59   |   0.04   \n","  20    |   0.190717   |  0.552168  |   70.59   |   0.04   \n","\n","\n","Training complete! Best accuracy: 79.41%.\n","2 -fold--------------\n","Start training...\n","\n"," Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","------------------------------------------------------------\n","   1    |   0.739436   |  0.682410  |   52.94   |   0.04   \n","   2    |   0.636298   |  0.678059  |   52.94   |   0.04   \n","   3    |   0.613113   |  0.665239  |   55.88   |   0.05   \n","   4    |   0.529942   |  0.680010  |   55.88   |   0.04   \n","   5    |   0.525543   |  0.689251  |   47.06   |   0.05   \n","   6    |   0.499581   |  0.632778  |   73.53   |   0.04   \n","   7    |   0.448433   |  0.607781  |   58.82   |   0.04   \n","   8    |   0.396327   |  0.615850  |   73.53   |   0.04   \n","   9    |   0.379219   |  0.600896  |   73.53   |   0.05   \n","  10    |   0.362544   |  0.576461  |   55.88   |   0.04   \n","  11    |   0.311753   |  0.555709  |   73.53   |   0.04   \n","  12    |   0.320009   |  0.560699  |   73.53   |   0.05   \n","  13    |   0.260112   |  0.537240  |   76.47   |   0.05   \n","  14    |   0.237356   |  0.525233  |   79.41   |   0.05   \n","  15    |   0.230239   |  0.517514  |   79.41   |   0.04   \n","  16    |   0.238496   |  0.522109  |   73.53   |   0.05   \n","  17    |   0.197300   |  0.507916  |   76.47   |   0.04   \n","  18    |   0.177091   |  0.507341  |   79.41   |   0.05   \n","  19    |   0.162782   |  0.501024  |   76.47   |   0.05   \n","  20    |   0.155076   |  0.496355  |   73.53   |   0.05   \n","\n","\n","Training complete! Best accuracy: 79.41%.\n","3 -fold--------------\n","Start training...\n","\n"," Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","------------------------------------------------------------\n","   1    |   0.726334   |  0.704878  |   50.00   |   0.04   \n","   2    |   0.631829   |  0.735213  |   47.06   |   0.05   \n","   3    |   0.598934   |  0.708226  |   50.00   |   0.05   \n","   4    |   0.531091   |  0.696840  |   55.88   |   0.04   \n","   5    |   0.528700   |  0.700983  |   52.94   |   0.05   \n","   6    |   0.503257   |  0.668401  |   58.82   |   0.06   \n","   7    |   0.429564   |  0.659475  |   52.94   |   0.05   \n","   8    |   0.386184   |  0.656692  |   64.71   |   0.04   \n","   9    |   0.380385   |  0.634338  |   61.76   |   0.04   \n","  10    |   0.323648   |  0.642986  |   55.88   |   0.04   \n","  11    |   0.313104   |  0.628166  |   55.88   |   0.05   \n","  12    |   0.320483   |  0.629641  |   64.71   |   0.05   \n","  13    |   0.255403   |  0.609201  |   61.76   |   0.05   \n","  14    |   0.234262   |  0.602798  |   55.88   |   0.05   \n","  15    |   0.249639   |  0.614393  |   58.82   |   0.05   \n","  16    |   0.197043   |  0.585794  |   58.82   |   0.05   \n","  17    |   0.185250   |  0.590255  |   58.82   |   0.05   \n","  18    |   0.173828   |  0.585249  |   61.76   |   0.05   \n","  19    |   0.155214   |  0.570895  |   61.76   |   0.04   \n","  20    |   0.192316   |  0.567324  |   64.71   |   0.04   \n","\n","\n","Training complete! Best accuracy: 64.71%.\n","4 -fold--------------\n","Start training...\n","\n"," Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","------------------------------------------------------------\n","   1    |   0.717323   |  0.703483  |   52.94   |   0.05   \n","   2    |   0.643483   |  0.734399  |   44.12   |   0.04   \n","   3    |   0.618907   |  0.671935  |   52.94   |   0.05   \n","   4    |   0.517914   |  0.663853  |   55.88   |   0.04   \n","   5    |   0.512325   |  0.673285  |   55.88   |   0.04   \n","   6    |   0.499990   |  0.642817  |   64.71   |   0.04   \n","   7    |   0.448034   |  0.641841  |   52.94   |   0.05   \n","   8    |   0.392425   |  0.638106  |   70.59   |   0.05   \n","   9    |   0.383473   |  0.613093  |   76.47   |   0.04   \n","  10    |   0.337831   |  0.627564  |   52.94   |   0.05   \n","  11    |   0.294167   |  0.584043  |   67.65   |   0.05   \n","  12    |   0.315845   |  0.582715  |   76.47   |   0.05   \n","  13    |   0.273001   |  0.567067  |   70.59   |   0.05   \n","  14    |   0.226483   |  0.562722  |   70.59   |   0.05   \n","  15    |   0.226292   |  0.571414  |   64.71   |   0.05   \n","  16    |   0.187007   |  0.557739  |   67.65   |   0.05   \n","  17    |   0.220594   |  0.565647  |   61.76   |   0.05   \n","  18    |   0.175019   |  0.559270  |   73.53   |   0.05   \n","  19    |   0.154421   |  0.548456  |   79.41   |   0.04   \n","  20    |   0.161187   |  0.552774  |   70.59   |   0.05   \n","\n","\n","Training complete! Best accuracy: 79.41%.\n","5 -fold--------------\n","Start training...\n","\n"," Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","------------------------------------------------------------\n","   1    |   0.737289   |  0.664859  |   58.82   |   0.04   \n","   2    |   0.621584   |  0.650702  |   64.71   |   0.04   \n","   3    |   0.609860   |  0.654281  |   70.59   |   0.04   \n","   4    |   0.539609   |  0.726002  |   44.12   |   0.04   \n","   5    |   0.523566   |  0.734294  |   44.12   |   0.05   \n","   6    |   0.500616   |  0.646254  |   61.76   |   0.04   \n","   7    |   0.435384   |  0.609158  |   58.82   |   0.05   \n","   8    |   0.397130   |  0.634758  |   64.71   |   0.04   \n","   9    |   0.376066   |  0.610964  |   70.59   |   0.04   \n","  10    |   0.387904   |  0.580978  |   58.82   |   0.05   \n","  11    |   0.302425   |  0.545715  |   73.53   |   0.04   \n","  12    |   0.310436   |  0.583054  |   73.53   |   0.05   \n","  13    |   0.258129   |  0.517293  |   70.59   |   0.05   \n","  14    |   0.232997   |  0.512994  |   73.53   |   0.05   \n","  15    |   0.249435   |  0.492727  |   76.47   |   0.04   \n","  16    |   0.206162   |  0.492273  |   70.59   |   0.04   \n","  17    |   0.194284   |  0.474871  |   73.53   |   0.05   \n","  18    |   0.175913   |  0.467159  |   76.47   |   0.05   \n","  19    |   0.154811   |  0.473975  |   70.59   |   0.04   \n","  20    |   0.194456   |  0.462962  |   67.65   |   0.04   \n","\n","\n","Training complete! Best accuracy: 76.47%.\n","6 -fold--------------\n","Start training...\n","\n"," Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","------------------------------------------------------------\n","   1    |   0.716552   |  0.701099  |   44.12   |   0.04   \n","   2    |   0.614229   |  0.699842  |   44.12   |   0.05   \n","   3    |   0.622116   |  0.651060  |   58.82   |   0.05   \n","   4    |   0.533263   |  0.614733  |   70.59   |   0.04   \n","   5    |   0.528247   |  0.613177  |   61.76   |   0.04   \n","   6    |   0.497541   |  0.577919  |   79.41   |   0.04   \n","   7    |   0.434335   |  0.566600  |   70.59   |   0.05   \n","   8    |   0.373359   |  0.533981  |   76.47   |   0.05   \n","   9    |   0.373306   |  0.523913  |   79.41   |   0.04   \n","  10    |   0.369550   |  0.580052  |   64.71   |   0.05   \n","  11    |   0.309568   |  0.516686  |   76.47   |   0.04   \n","  12    |   0.325685   |  0.486955  |   79.41   |   0.05   \n","  13    |   0.271918   |  0.480686  |   79.41   |   0.04   \n","  14    |   0.238786   |  0.472432  |   79.41   |   0.05   \n","  15    |   0.241092   |  0.480763  |   76.47   |   0.04   \n","  16    |   0.196497   |  0.458900  |   79.41   |   0.04   \n","  17    |   0.183913   |  0.469278  |   82.35   |   0.05   \n","  18    |   0.173277   |  0.464579  |   82.35   |   0.05   \n","  19    |   0.154241   |  0.436201  |   79.41   |   0.04   \n","  20    |   0.198342   |  0.463732  |   76.47   |   0.04   \n","\n","\n","Training complete! Best accuracy: 82.35%.\n","7 -fold--------------\n","Start training...\n","\n"," Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","------------------------------------------------------------\n","   1    |   0.694588   |  0.712075  |   45.45   |   0.05   \n","   2    |   0.659094   |  0.717367  |   45.45   |   0.04   \n","   3    |   0.607358   |  0.668431  |   63.64   |   0.04   \n","   4    |   0.537223   |  0.655331  |   66.67   |   0.04   \n","   5    |   0.517790   |  0.626359  |   66.67   |   0.05   \n","   6    |   0.458712   |  0.628144  |   69.70   |   0.04   \n","   7    |   0.435688   |  0.643388  |   60.61   |   0.05   \n","   8    |   0.399695   |  0.568382  |   69.70   |   0.05   \n","   9    |   0.328487   |  0.544682  |   84.85   |   0.04   \n","  10    |   0.312971   |  0.540286  |   75.76   |   0.05   \n","  11    |   0.303410   |  0.526742  |   78.79   |   0.04   \n","  12    |   0.264358   |  0.510744  |   78.79   |   0.05   \n","  13    |   0.232064   |  0.496203  |   84.85   |   0.05   \n","  14    |   0.222470   |  0.481162  |   81.82   |   0.05   \n","  15    |   0.183797   |  0.471533  |   84.85   |   0.04   \n","  16    |   0.177379   |  0.469646  |   81.82   |   0.04   \n","  17    |   0.179407   |  0.456397  |   81.82   |   0.05   \n","  18    |   0.146307   |  0.452866  |   84.85   |   0.05   \n","  19    |   0.151376   |  0.448774  |   84.85   |   0.04   \n","  20    |   0.122819   |  0.447327  |   81.82   |   0.05   \n","\n","\n","Training complete! Best accuracy: 84.85%.\n","8 -fold--------------\n","Start training...\n","\n"," Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","------------------------------------------------------------\n","   1    |   0.688839   |  0.724031  |   48.48   |   0.04   \n","   2    |   0.657601   |  0.737977  |   48.48   |   0.04   \n","   3    |   0.603262   |  0.677663  |   66.67   |   0.04   \n","   4    |   0.525611   |  0.664201  |   66.67   |   0.05   \n","   5    |   0.516534   |  0.640782  |   57.58   |   0.04   \n","   6    |   0.478538   |  0.635234  |   75.76   |   0.05   \n","   7    |   0.417853   |  0.661366  |   60.61   |   0.05   \n","   8    |   0.391186   |  0.608921  |   63.64   |   0.05   \n","   9    |   0.333114   |  0.582654  |   69.70   |   0.04   \n","  10    |   0.309221   |  0.591974  |   78.79   |   0.04   \n","  11    |   0.274163   |  0.574032  |   63.64   |   0.04   \n","  12    |   0.253287   |  0.558569  |   75.76   |   0.05   \n","  13    |   0.236638   |  0.561879  |   69.70   |   0.05   \n","  14    |   0.222049   |  0.549599  |   69.70   |   0.04   \n","  15    |   0.185188   |  0.540390  |   66.67   |   0.04   \n","  16    |   0.172361   |  0.541666  |   66.67   |   0.05   \n","  17    |   0.154208   |  0.542554  |   66.67   |   0.05   \n","  18    |   0.135443   |  0.539210  |   69.70   |   0.04   \n","  19    |   0.127467   |  0.537868  |   69.70   |   0.05   \n","  20    |   0.113461   |  0.537949  |   75.76   |   0.05   \n","\n","\n","Training complete! Best accuracy: 78.79%.\n","9 -fold--------------\n","Start training...\n","\n"," Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","------------------------------------------------------------\n","   1    |   0.692855   |  0.734208  |   42.42   |   0.04   \n","   2    |   0.655901   |  0.743317  |   42.42   |   0.05   \n","   3    |   0.599156   |  0.681859  |   51.52   |   0.05   \n","   4    |   0.531549   |  0.653087  |   72.73   |   0.04   \n","   5    |   0.511076   |  0.625162  |   72.73   |   0.04   \n","   6    |   0.452125   |  0.625046  |   72.73   |   0.04   \n","   7    |   0.424367   |  0.657176  |   63.64   |   0.04   \n","   8    |   0.397042   |  0.572873  |   78.79   |   0.05   \n","   9    |   0.323264   |  0.564713  |   69.70   |   0.05   \n","  10    |   0.299512   |  0.581750  |   66.67   |   0.04   \n","  11    |   0.258025   |  0.544667  |   69.70   |   0.04   \n","  12    |   0.251588   |  0.561275  |   66.67   |   0.05   \n","  13    |   0.232318   |  0.532970  |   72.73   |   0.05   \n","  14    |   0.204172   |  0.538478  |   72.73   |   0.04   \n","  15    |   0.178696   |  0.533604  |   72.73   |   0.04   \n","  16    |   0.168832   |  0.525803  |   72.73   |   0.04   \n","  17    |   0.154120   |  0.526542  |   72.73   |   0.05   \n","  18    |   0.130361   |  0.528253  |   72.73   |   0.05   \n","  19    |   0.132254   |  0.522452  |   75.76   |   0.04   \n","  20    |   0.115958   |  0.544118  |   69.70   |   0.05   \n","\n","\n","Training complete! Best accuracy: 78.79%.\n","10 -fold--------------\n","Start training...\n","\n"," Epoch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n","------------------------------------------------------------\n","   1    |   0.691194   |  0.711960  |   48.48   |   0.04   \n","   2    |   0.662296   |  0.722377  |   48.48   |   0.05   \n","   3    |   0.602943   |  0.682883  |   48.48   |   0.05   \n","   4    |   0.532116   |  0.660758  |   63.64   |   0.04   \n","   5    |   0.509535   |  0.638283  |   72.73   |   0.05   \n","   6    |   0.455929   |  0.634057  |   60.61   |   0.05   \n","   7    |   0.433021   |  0.639263  |   60.61   |   0.04   \n","   8    |   0.388849   |  0.593712  |   60.61   |   0.05   \n","   9    |   0.321029   |  0.565804  |   78.79   |   0.05   \n","  10    |   0.304967   |  0.557936  |   69.70   |   0.04   \n","  11    |   0.290757   |  0.552465  |   81.82   |   0.04   \n","  12    |   0.262137   |  0.534100  |   66.67   |   0.05   \n","  13    |   0.243719   |  0.516931  |   81.82   |   0.05   \n","  14    |   0.215104   |  0.507502  |   75.76   |   0.04   \n","  15    |   0.188548   |  0.502670  |   78.79   |   0.04   \n","  16    |   0.168855   |  0.495486  |   78.79   |   0.04   \n","  17    |   0.167342   |  0.495447  |   75.76   |   0.05   \n","  18    |   0.144535   |  0.489456  |   75.76   |   0.05   \n","  19    |   0.140554   |  0.483222  |   78.79   |   0.05   \n","  20    |   0.119573   |  0.483275  |   72.73   |   0.05   \n","\n","\n","Training complete! Best accuracy: 81.82%.\n","78.60071301247771\n"]}]},{"cell_type":"code","source":["\"\"\"\n","set_seed(42)\n","\n","cnn_rand, optimizer = initilize_model(vocab_size=len(word2idx),\n","                                      embed_dim=300,\n","                                      learning_rate=0.25,\n","                                      dropout=0.5)\n","train(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=20)\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"id":"j9jSyIH8L0fP","executionInfo":{"status":"ok","timestamp":1652795866372,"user_tz":-120,"elapsed":28,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}},"outputId":"e4c3c30b-38c5-465f-e753-6881ff26f02e"},"execution_count":246,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nset_seed(42)\\n\\ncnn_rand, optimizer = initilize_model(vocab_size=len(word2idx),\\n                                      embed_dim=300,\\n                                      learning_rate=0.25,\\n                                      dropout=0.5)\\ntrain(cnn_rand, optimizer, train_dataloader, val_dataloader, epochs=20)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":246}]},{"cell_type":"code","source":["\"\"\"\n","# CNN-static: fastText pretrained word vectors are used and freezed during training.\n","set_seed(42)\n","cnn_static, optimizer = initilize_model(pretrained_embedding=embeddings,\n","                                        freeze_embedding=True,\n","                                        learning_rate=0.25,\n","                                        dropout=0.5)\n","train(cnn_static, optimizer, train_dataloader, val_dataloader, epochs=20)\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"ny8YkA3xL2tX","executionInfo":{"status":"ok","timestamp":1652795866373,"user_tz":-120,"elapsed":24,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}},"outputId":"a0299e76-5d38-4a2b-f99c-4b387ebb1b92"},"execution_count":247,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# CNN-static: fastText pretrained word vectors are used and freezed during training.\\nset_seed(42)\\ncnn_static, optimizer = initilize_model(pretrained_embedding=embeddings,\\n                                        freeze_embedding=True,\\n                                        learning_rate=0.25,\\n                                        dropout=0.5)\\ntrain(cnn_static, optimizer, train_dataloader, val_dataloader, epochs=20)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":247}]},{"cell_type":"code","source":["\"\"\"\n","# CNN-non-static: fastText pretrained word vectors are fine-tuned during training.\n","set_seed(42)\n","cnn_non_static, optimizer = initilize_model(pretrained_embedding=embeddings,\n","                                            freeze_embedding=False,\n","                                            learning_rate=0.25,\n","                                            dropout=0.5)\n","train(cnn_non_static, optimizer, train_dataloader, val_dataloader, epochs=20)\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"QEYgAXpDM4Ge","executionInfo":{"status":"ok","timestamp":1652795866731,"user_tz":-120,"elapsed":377,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}},"outputId":"43cfe157-a295-4f3c-fc30-7fcaf231e86c"},"execution_count":248,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# CNN-non-static: fastText pretrained word vectors are fine-tuned during training.\\nset_seed(42)\\ncnn_non_static, optimizer = initilize_model(pretrained_embedding=embeddings,\\n                                            freeze_embedding=False,\\n                                            learning_rate=0.25,\\n                                            dropout=0.5)\\ntrain(cnn_non_static, optimizer, train_dataloader, val_dataloader, epochs=20)\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":248}]},{"cell_type":"code","source":["\n","def predict(text, model=cnn_non_static.to(\"cpu\"), max_len=140): #max_len also modified by me\n","    \"\"\"Predict probability that the label is 1.\"\"\"\n","\n","    # Tokenize, pad and encode text\n","    tokens = word_tokenize(text.lower())\n","    padded_tokens = tokens + ['<pad>'] * (max_len - len(tokens))\n","    input_id = [word2idx.get(token, word2idx['<unk>']) for token in padded_tokens]\n","\n","    # Convert to PyTorch tensors\n","    input_id = torch.tensor(input_id).unsqueeze(dim=0)\n","\n","    # Compute logits\n","    logits = model.forward(input_id)\n","\n","    #  Compute probability\n","    probs = F.softmax(logits, dim=1).squeeze(dim=0)\n","\n","    print(f\"This tweet is {probs[1] * 100:.2f}% 1.\")\n"],"metadata":{"id":"AYCM_PmWOZD9","executionInfo":{"status":"ok","timestamp":1652795866732,"user_tz":-120,"elapsed":13,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}}},"execution_count":249,"outputs":[]},{"cell_type":"code","source":["\n","#tt = \"Keine Zwangsimpfung für Kinder\"\n","#predict(tt)\n"],"metadata":{"id":"NmwGJ69POaeg","executionInfo":{"status":"ok","timestamp":1652795866735,"user_tz":-120,"elapsed":14,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}}},"execution_count":250,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"UPgAy1iHOf0q","executionInfo":{"status":"ok","timestamp":1652795866736,"user_tz":-120,"elapsed":14,"user":{"displayName":"Xixuan Zhang","userId":"02440855261758327636"}}},"execution_count":250,"outputs":[]}]}